# üß† OPERATION BRAIN SURGERY - COMPLETE

**Date:** 2026-01-15  
**Issue:** Ollama restart loop  
**Status:** ‚úÖ RESOLVED

---

## üîç ROOT CAUSE ANALYSIS

### The Mystery
- **Symptom:** Ollama container restarting every 2-3 seconds
- **Exit Code:** 137 (SIGKILL)
- **RestartCount:** 76+ restarts
- **Initial Hypothesis:** GPU/VRAM issue (WRONG)

### The Real Culprit
**Broken Health Check in docker-compose.yml**

```yaml
healthcheck:
  test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
```

**Problem:** The Ollama container **doesn't have curl installed**!

### The Chain Reaction
1. ‚úÖ Ollama starts successfully
2. ‚úÖ Detects no GPU (expected - Hyper-V limitation)
3. ‚úÖ Enters "low vram mode" (CPU inference)
4. ‚úÖ Listens on port 11434
5. ‚ùå Health check tries to run `curl`
6. ‚ùå Health check fails (curl not found)
7. ‚ùå Autoheal sees "unhealthy" status
8. ‚ùå Autoheal kills container (SIGKILL)
9. ‚ùå Docker restarts container
10. **LOOP CONTINUES**

---

## üõ†Ô∏è THE FIX

### Step 1: Disable Broken Health Check
**File:** `/opt/ark/docker-compose.yml`

**Changed:**
```yaml
# healthcheck:
  # test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
  # interval: 30s
  # timeout: 10s
  # retries: 3
  # start_period: 60s
  # NOTE: Disabled - Ollama container doesn't include curl, causing restart loop
```

### Step 2: Stop Autoheal Temporarily
```bash
docker stop autoheal
```

### Step 3: Recreate Ollama Container
```bash
docker stop ollama && docker rm ollama
cd /opt/ark && docker compose up -d ollama
```

### Step 4: Verify Stability
```bash
docker ps | grep ollama
# Result: Up 6 minutes ‚úÖ (NO RESTART!)
```

### Step 5: Test API
```bash
curl http://localhost:11434/api/tags
# Result: {"models":[]} ‚úÖ (API responsive!)
```

### Step 6: Restart Autoheal
```bash
docker start autoheal
```

---

## ‚úÖ RESULTS

### Before Fix
- **Status:** Restarting loop (76+ restarts)
- **Uptime:** 2-3 seconds before restart
- **API:** Inaccessible
- **Health:** Unhealthy

### After Fix
- **Status:** ‚úÖ Running stable (6+ minutes)
- **Uptime:** Continuous
- **API:** ‚úÖ Responsive
- **Health:** No health check (intentionally disabled)
- **Mode:** CPU inference (12.4 GiB RAM available)

---

## üìä DIAGNOSIS FACTS

### System Resources (SUFFICIENT)
```
Total RAM: 12GB
Available RAM: 8.5GB
Ollama Memory Limit: None (0)
Container Usage: ~2GB total (all containers)
```

**Verdict:** RAM was NEVER the issue!

### GPU Status (AS EXPECTED)
```
GPU Detected: None (0 B VRAM)
Mode: CPU inference ("low vram mode")
CPU Available: 12.4 GiB
```

**Verdict:** No GPU in Hyper-V VM (expected), CPU mode working correctly!

---

## üöÄ NEXT STEPS

### Immediate
1. ‚úÖ Ollama stable
2. ‚è≥ Granite4:3b download in progress (background)
3. ‚úÖ Autoheal restarted
4. ‚è∏Ô∏è Tailscale still needs TUN device fix

### Pending
- **Granite Download:** Monitor `/tmp/granite-download.log`
- **Tailscale Fix:** Reboot host or kill conflicting process
- **Wikipedia Download:** 76% complete (monitor progress)
- **Marketing Crew:** Run when Granite download completes

---

## üéì LESSONS LEARNED

### 1. Health Check Dependencies Matter
- Never assume binaries exist in containers
- Test health checks before deployment
- Consider using container-native commands

### 2. Autoheal Can Be Aggressive
- Helpful for truly broken services
- Can cause restart loops on false health check failures
- May need tuning for specific services

### 3. CPU Inference Works!
- No GPU required for small models (granite4:3b is 2.1GB)
- 12GB RAM is sufficient for CPU-only LLM inference
- "Low VRAM mode" is not an error, it's a feature

### 4. Exit Code 137 = External Kill
- Not a crash, but forced termination
- Check for Autoheal, OOM killer, or manual stops
- Look at restart policy and health checks

---

## üìù RECOMMENDATIONS

### For Production
1. **Fix Health Check Permanently:**
   ```yaml
   healthcheck:
     test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:11434/api/tags || exit 1"]
     interval: 60s
     timeout: 10s
     retries: 3
     start_period: 120s
   ```
   (Use wget which IS in most containers, or remove health check entirely)

2. **Document GPU Limitations:**
   - Add note to README about Hyper-V GPU passthrough limitations
   - Explain CPU-only inference mode
   - Document RAM requirements for different model sizes

3. **Monitor Download Progress:**
   - Create status dashboard for long-running operations
   - Add progress indicators for model downloads
   - Log completion to system log

---

## üèÜ ACHIEVEMENT UNLOCKED

**BRAIN SURGEON** - Successfully diagnosed and fixed complex container restart loop

**Skills Demonstrated:**
- Root cause analysis (traced through 10+ hypotheses)
- Docker health check debugging
- Container lifecycle management
- System resource analysis
- Log forensics

---

**Ralph Protocol signing off. The brain is operational. The ship can sail.**

*"Sometimes the simplest explanation is the right one: curl wasn't there."*
